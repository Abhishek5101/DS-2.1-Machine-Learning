{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Images/slide_1.png\" width=\"700\" height=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Images/slide_2.png\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree\n",
    "\n",
    "- Decision Trees are considered one of the most mature, traditional algorithms in predictive analytics\n",
    "\n",
    "- They are most likely used for classification problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why and when we need Decision Tree ?\n",
    "\n",
    "- When features are Categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Examples of Decision Tree\n",
    "\n",
    "- You’ll take a small dataset and see if you can learn anything from it\n",
    "\n",
    "- You’ll see if a decision tree can give you any insight as to how the eye doctor prescribes contact lenses\n",
    "\n",
    "- You can predict the type of lenses people will use and understand the underlying processes with a decision tree\n",
    "\n",
    "- Predict does a player plays tennis outside based on weather conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lens Dataset\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    " 3 Classes\n",
    "\n",
    " 1 : the patient should be fitted with hard contact lenses,\n",
    "\n",
    " 2 : the patient should be fitted with soft contact lenses,\n",
    "\n",
    " 3 : the patient should not be fitted with contact lenses.\n",
    " \n",
    "4 Features\n",
    "\n",
    "1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic\n",
    "\n",
    "2. spectacle prescription:  (1) myope, (2) hypermetrope\n",
    "\n",
    "3. astigmatic:     (1) no, (2) yes\n",
    "\n",
    "4. tear production rate:  (1) reduced, (2) normal\n",
    "\n",
    "<img src=\"Images/lens_data.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Images/lens_DT.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The root and the leafs for Decision Tree are obtained based on: \n",
    "\n",
    "- Conditional Probability\n",
    "\n",
    "- Entropy\n",
    "\n",
    "- Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees are based on Entropy\n",
    "\n",
    "## Activity: Calculate the entropy for a coin\n",
    "\n",
    "Entropy shows the uncertainy about a random variable\n",
    "\n",
    "Entropy formula is the summation of probabilities times log of probabilities\n",
    "\n",
    "`Entropy(Coin) = sum(-p(outcome)xlog2p(outcome) for outcome in [H, T])`\n",
    "\n",
    "for a fair coin: `Entropy(Coin) = sum(-p(outcome)xlog2p(outcome) for p(outcome) in [p(H)=1/2, p(T)=1/2])`\n",
    "\n",
    "Show that the fair coin has the largest entropy (uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p):\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H\n",
    "    \n",
    "p = [.5, .5]\n",
    "print(entropy(p))\n",
    "\n",
    "p = [.9, .1]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Change p (probability of head and tail) and plot entropy for different p\n",
    "\n",
    "<img src=\"coin_entropy.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    " The fair coin has the highest entropy which means a fair coin has the highest uncertain result when toss a coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let build a Decision Tree for Tennis Data\n",
    "\n",
    "The following table informs about decision making factors to play tennis at outside based on 14 days data, for different weather conditions\n",
    "\n",
    "<img src=\"dst_1.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity: Obtain the following quantitites:\n",
    "\n",
    "- Obtain the entropy of `PlayTennis` (Decision) column. \n",
    "\n",
    "Hint: p = [9/14, 5/14] which represents the probability that a player plays tennis or not\n",
    "\n",
    "`Entropy(Decision) = – (9/14) . log2(9/14) – (5/14) . log2(5/14) = 0.940`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Wind factor on Decision\n",
    "\n",
    "    - Obtain the entropy of conditional probability `p(PlayTennis | Wind = Weak) = [2/8, 6/8]`\n",
    "    \n",
    "    - <img src=\"weak_wind_decision.png\" width=\"400\" height=\"400\">\n",
    "    \n",
    "    - Obtain the entropy of conditional probability `p(PlayTennis | Wind = Strong) = [3/6, 3/6]`\n",
    "    \n",
    "    -  <img src=\"strong_wind_decision.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtain the Information Gain Between PlayTennis (Decision) and Wind\n",
    "\n",
    "- What is the probability that wind be weak? \n",
    "\n",
    "Hint = Count how many weak wind we have devide over how many sample we have.\n",
    "\n",
    "`p(Wind = Weak) = 8/ 14`\n",
    "\n",
    "`p(Wind = Strong) = 6/ 14`\n",
    "\n",
    "<img src=\"Information_Gain.png\" width=\"=500\" height=\"500\">\n",
    "\n",
    "<img src=\"Conditional_Entropy.png\" width=\"=500\" height=\"500\">\n",
    "\n",
    "- https://pdfs.semanticscholar.org/26e0/a38b6a81802d9d3c7fdd430befda7ea6bf11.pdf\n",
    "\n",
    "- Information Gain(Decision, Wind) = \n",
    "\n",
    "`Entropy(Decision) - sum(Entropy(Decision | Wind ) p(Wind)) for Wind = {Weak, Strong}`\n",
    "\n",
    "`Entropy(Decision) - p(Wind = Weak)Entropy(Decision | Wind = Weak ) - p(Wind = Strong)Entropy(Decision | Wind = Strong )`\n",
    "\n",
    "= 0.048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other factors on Decision column\n",
    "\n",
    "We have applied similar calculation on the other features (columns)\n",
    "\n",
    "1 - Information Gain(Decision, Wind) = 0.048\n",
    "\n",
    "2 - Information Gain(Decision, Outlook) = 0.246\n",
    "\n",
    "3 - Information Gain(Decision, Temperature) = 0.029\n",
    "\n",
    "4 - Information Gain(Decision, Humidity) = 0.151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see Outlook and Decision has the highest Gain, so Outlook would be the root for the Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If keep continuing the calculation of Information Gain between nodes (features), \n",
    "\n",
    "### The tree is built based on the highest Information Gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity: Build the decision tree with sklearn for tennis dataset\n",
    "\n",
    "#### For Decision Tree Visualization in Python:\n",
    "\n",
    "Packages that are needed:\n",
    "\n",
    "`conda install -c anaconda graphviz`\n",
    "\n",
    "`conda install -c anaconda pydot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "\n",
    "data = pd.read_csv('tennis.txt', delimiter=\"\\t\", header=None, names=['Outlook', 'Temp', 'Humidity', 'Wind', 'Play'])\n",
    "print(data)\n",
    "\n",
    "data_encoded = data.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "print(data_encoded)\n",
    "\n",
    "#\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "\n",
    "# one_hot_data = pd.get_dummies(data[['a', 'b', 'c', 'd']], drop_first=True)\n",
    "# print(one_hot_data)\n",
    "clf.fit(data_encoded[['Outlook', 'Temp', 'Humidity', 'Wind']], data_encoded['Play'])\n",
    "\n",
    "\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=['Outlook', 'Temp.', 'Humidity', 'Wind'])\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_png('tennis_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Write a function that takes Tennis pandas dataframe, one feature (for example Wind) and target column (Decision)\n",
    "\n",
    "- then returns information gain between the feature and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook  Temp Humidity    Wind Play\n",
      "1      Sunny   Hot     High    Weak   No\n",
      "2      Sunny   Hot     High  Strong   No\n",
      "3   Overcast   Hot     High    Weak  Yes\n",
      "4       Rain  Mild     High    Weak  Yes\n",
      "5       Rain  Cool   Normal    Weak  Yes\n",
      "6       Rain  Cool   Normal  Strong   No\n",
      "7   Overcast  Cool   Normal  Strong  Yes\n",
      "8      Sunny  Mild     High    Weak   No\n",
      "9      Sunny  Cool   Normal    Weak  Yes\n",
      "10      Rain  Mild   Normal    Weak  Yes\n",
      "11     Sunny  Mild   Normal  Strong  Yes\n",
      "12  Overcast  Mild     High  Strong  Yes\n",
      "13  Overcast   Hot   Normal    Weak  Yes\n",
      "14      Rain  Mild     High  Strong   No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tennis.txt', delimiter=\"\\t\", header=None, names=['Outlook', 'Temp', 'Humidity', 'Wind', 'Play'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: helper function for entopy\n",
    "import numpy as np\n",
    "\n",
    "def entropy(p):\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No': 0.25, 'Yes': 0.75}\n",
      "{'No': 0.5, 'Yes': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# hint: helper functyion for a feature and decision and condition of the feature\n",
    "def conditional_prob(df, c1, c2, condition):\n",
    "    df_new = df[df[c1] == condition][c2]\n",
    "    s = df_new.unique()\n",
    "    population_size = len(df_new)\n",
    "    pr = {}\n",
    "    for i in s:\n",
    "        pr[i] = len(df[(df[c1] == condition) & (df[c2]== i)]) / population_size\n",
    "    \n",
    "    return pr\n",
    "        \n",
    "print(conditional_prob(data,'Wind', 'Play', 'Weak'))\n",
    "print(conditional_prob(data, 'Wind', 'Play', 'Strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(df, feature, decision):\n",
    "    dict_decision = dict(df[decision].value_counts())\n",
    "    prob_decision = [q for (p,q) in dict_decision.items()]/sum(dict_decision.values())\n",
    "    entropy_decision = entropy(prob_decision)\n",
    "#     print(entropy_decision)\n",
    "    \n",
    "    dict_feature = dict(df[feature].value_counts())\n",
    "    dict_prob_feature = {}\n",
    "    for (p,q) in dict_feature.items():\n",
    "        dict_prob_feature[p] = q/sum(dict_feature.values())\n",
    "#     print(dict_prob_feature)\n",
    "    \n",
    "    conditions = df[feature].unique()\n",
    "    dict_ = {}\n",
    "    for condition in conditions:\n",
    "        dict_[condition] = conditional_prob(df, feature, decision, condition)\n",
    "#     print(dict_)\n",
    "    S = 0\n",
    "    for (i,j) in dict_.items():\n",
    "#         print(i,j)\n",
    "        prob_condition = list(dict_[i].values())\n",
    "#         print(entropy_condition)\n",
    "        S = S + dict_prob_feature[i]*entropy(prob_condition)\n",
    "#         print(dict_prob_feature[i]*entropy(entropy_condition))\n",
    "    print(entropy_decision - S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04812703040826949\n",
      "0.15183550136234159\n",
      "0.02922256565895487\n",
      "0.24674981977443933\n"
     ]
    }
   ],
   "source": [
    "info_gain(data, 'Wind', 'Play')\n",
    "info_gain(data, 'Humidity', 'Play')\n",
    "info_gain(data, 'Temp', 'Play')\n",
    "info_gain(data, 'Outlook', 'Play')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: We can show the information gain between any feature with it self is equal to its entropy: \n",
    "\n",
    "- $I(X, X) = H(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5774062828523454\n",
      "1.5774062828523454\n",
      "1.5566567074628228\n",
      "1.5566567074628228\n",
      "1.0\n",
      "1.0\n",
      "0.9852281360342515\n",
      "0.9852281360342515\n",
      "0.9402859586706311\n",
      "0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "for i in ['Outlook', 'Temp', 'Humidity', 'Wind', 'Play']:\n",
    "    p = [m/sum(data[i].value_counts().to_dict().values()) for m in list(data[i].value_counts().to_dict().values())]\n",
    "    print(entropy(p))\n",
    "    info_gain(data, i, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "- A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences\n",
    "\n",
    "- Decision Tree provides an effective method of Decision Making \n",
    "\n",
    "- The best characteristic of using trees for analytics -> easy to interpret and explain to executives\n",
    "\n",
    "- The mechanism behind DT are maximizing information gains while have different options "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources:\n",
    "\n",
    "- https://sefiks.com/2017/11/20/a-step-by-step-id3-decision-tree-example/\n",
    "\n",
    "- https://heartbeat.fritz.ai/introduction-to-decision-tree-learning-cd604f85e236\n",
    "\n",
    "- https://journals.plos.org/plosone/article/figure/image?id=10.1371/journal.pone.0161696.g002&size=inline\n",
    "\n",
    "DT Visualization:\n",
    "\n",
    "- https://explained.ai/decision-tree-viz/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mild': 6, 'Cool': 4, 'Hot': 4}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Temp'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4, 4]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['Temp'].value_counts().to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data['Temp'].value_counts().to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5566567074628228\n"
     ]
    }
   ],
   "source": [
    "p = [m/sum(data['Temp'].value_counts().to_dict().values()) for m in list(data['Temp'].value_counts().to_dict().values())]\n",
    "\n",
    "print(entropy(p))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
